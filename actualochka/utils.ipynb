{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Define the function to create LSTM dataset and save it\n",
    "def create_and_save_lstm_dataset(file_id, n_intervals, datasets_folder='../datasets', scalers_folder='../scalers'):\n",
    "    file_path = f'../data_updated/{file_names[file_id]}'\n",
    "    dataset = pd.read_csv(file_path).dropna()\n",
    "\n",
    "    target_column = dataset.pop('Цена')\n",
    "    dates = dataset.pop('Дата')\n",
    "    dataset['Цена'] = target_column\n",
    "\n",
    "    normalized_dataset = MinMaxScaler(feature_range=(0, 1)).fit_transform(dataset.values)\n",
    "\n",
    "    X_ltsm, y_ltsm = create_lstm_dataset(normalized_dataset, n_intervals, forecast_days=1)\n",
    "\n",
    "    train_size = int(len(X_ltsm) * 0.8)\n",
    "    X_train_ltsm, X_test_ltsm = X_ltsm[:train_size], X_ltsm[train_size:]\n",
    "    y_train_ltsm, y_test_ltsm = y_ltsm[:train_size], y_ltsm[train_size:]\n",
    "\n",
    "    # Save scalers\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(X_train_ltsm.reshape(-1, X_train_ltsm.shape[-1]))\n",
    "    scaler_filename = os.path.join(scalers_folder, f'{file_names[file_id]}_scaler.pkl')\n",
    "    with open(scaler_filename, 'wb') as scaler_file:\n",
    "        joblib.dump(scaler, scaler_file)\n",
    "\n",
    "    # Save datasets\n",
    "    X_train_ltsm_file_path = os.path.join(datasets_folder, f'X_train_ltsm_{file_names[file_id]}.npy')\n",
    "    X_test_ltsm_file_path = os.path.join(datasets_folder, f'X_test_ltsm_{file_names[file_id]}.npy')\n",
    "    y_train_ltsm_file_path = os.path.join(datasets_folder, f'y_train_ltsm_{file_names[file_id]}.npy')\n",
    "    y_test_ltsm_file_path = os.path.join(datasets_folder, f'y_test_ltsm_{file_names[file_id]}.npy')\n",
    "\n",
    "    np.save(X_train_ltsm_file_path, X_train_ltsm)\n",
    "    np.save(X_test_ltsm_file_path, X_test_ltsm)\n",
    "    np.save(y_train_ltsm_file_path, y_train_ltsm)\n",
    "    np.save(y_test_ltsm_file_path, y_test_ltsm)\n",
    "\n",
    "    print(f'Data saved to {X_train_ltsm_file_path} and {X_test_ltsm_file_path}')\n",
    "    print(f'Data saved to {y_train_ltsm_file_path} and {y_test_ltsm_file_path}')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from numpy import sqrt\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "from free_utils import file_names, n_intervals\n",
    "\n",
    "# Функция для формирования строки с описанием архитектуры модели\n",
    "def get_model_architecture(model):\n",
    "    architecture = \"\"\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, LSTM):\n",
    "            architecture += f\"LSTM_{layer.units}x{layer.return_sequences}_\"\n",
    "        elif isinstance(layer, Dense):\n",
    "            architecture += f\"Dense{layer.units}_\"\n",
    "    return architecture[:-1]\n",
    "\n",
    "# Функция для обучения LSTM модели и прогнозирования на следующие 300 дней\n",
    "def train_lstm_model_and_forecast(file_id, n_intervals, n_features, models_dir='models', data_folder='../datasets'):\n",
    "    # Загрузка данных\n",
    "    scaler_filename = '../scalers/' + file_names[file_id] + '_scaler.pkl'\n",
    "    with open(scaler_filename, 'rb') as scaler_file:\n",
    "        scaler = joblib.load(scaler_file)\n",
    "\n",
    "    file_paths = ['X_test_ltsm_', 'X_train_ltsm_', 'y_test_ltsm_', 'y_train_ltsm_']\n",
    "    data = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        file_name = file_path + file_names[file_id] + '.npy'\n",
    "        full_path = os.path.join(data_folder, file_name)\n",
    "        data.append(np.load(full_path))\n",
    "\n",
    "    X_test_ltsm, X_train_ltsm, y_test_ltsm, y_train_ltsm = data\n",
    "\n",
    "    # Создание модели\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=(X_train_ltsm.shape[1], X_train_ltsm.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=100, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=100, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Определение колбеков\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model_filename = os.path.join(models_dir, f'{file_names[file_id]}_{get_model_architecture(model)}_best_model.h5')\n",
    "    model_checkpoint = ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    # Обучение модели\n",
    "    history = model.fit(X_train_ltsm, y_train_ltsm,\n",
    "                        epochs=30, batch_size=32,\n",
    "                        validation_data=(X_test_ltsm, y_test_ltsm),\n",
    "                        callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    # Визуализация процесса обучения\n",
    "    plt.plot(history.history['loss'][3:], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'][3:], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Прогноз на следующие 300 дней\n",
    "    forecast_days = n_intervals\n",
    "    forecast = []\n",
    "    last_60_days = X_test_ltsm[-n_intervals:]\n",
    "\n",
    "    for i in range(forecast_days):\n",
    "        next_day_prediction = model.predict(last_60_days)\n",
    "        forecast.append(next_day_prediction[0, 0])\n",
    "        last_60_days = np.roll(last_60_days, shift=-1, axis=0)\n",
    "        last_60_days[-1] = next_day_prediction\n",
    "\n",
    "    # Обратное масштабирование прогнозов\n",
    "    forecast = np.array(forecast)\n",
    "    forecast = np.repeat(forecast, n_features).reshape(-1, n_features)\n",
    "    forecast = scaler.inverse_transform(forecast)[:, 0]\n",
    "\n",
    "    # Результаты прогнозирования\n",
    "    print(\"Прогнозы на следующие 300 дней:\")\n",
    "    print(forecast)\n",
    "\n",
    "    # Визуализация результатов\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(inv_y_train), len(inv_y_train) + len(inv_y_test)), inv_y_test, label='Actual (Test)')\n",
    "    plt.plot(range(len(inv_y_train), len(inv_y_train) + len(inv_y_test)), inv_yhat, label='Predicted (Test)')\n",
    "    plt.plot(range(len(inv_y_train) + len(inv_y_test), len(inv_y_train) + len(inv_y_test) + len(forecast)), forecast, label='Forecast (Next 100 days)')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Usage example\n",
    "file_id = 4\n",
    "n_intervals = 60\n",
    "n_features = 18\n",
    "create_and_save_lstm_dataset(file_id, n_intervals)\n",
    "# Использование функции\n",
    "train_lstm_model_and_forecast(file_id, n_intervals, n_features)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f329bac0d93416"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
